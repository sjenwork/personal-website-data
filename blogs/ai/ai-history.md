# 大型語言模型發展歷史時間線

*發布日期：2024-03-15*

*標籤：Agent, RAG*

![agent]([blogs/images/ai-history.png](https://github.com/sjenwork/personal-website-data/blob/main/blogs/images/ai-history.png?raw=true))

此文章記錄了ChatGPT(OpenAI)、Gemini(Google)和Claude(Anthropic)三大語言模型的發展歷史與方向，並涵蓋了其他重要模型與技術突破，特別聚焦於近期AI Agent的發展趨勢。

## 早期基礎（2017-2019）

### 2017年
- **6月**: **[Google]** 發佈 **Transformer** 模型，引入了注意力機制，為後續的大型語言模型奠定基礎。

### 2018年
- **6月**: **[OpenAI]** 發佈 **GPT-1**，首個將Transformer架構與無監督預訓練結合的大型語言模型，參數量約1.17億。
- **10月**: **[Google]** 發佈 **BERT**，改進了雙向編碼表示，成為自然語言處理的重要里程碑。

### 2019年
- **2月**: **[OpenAI]** 發佈 **GPT-2**，參數量達15億，顯著增強了文本生成的質量和多樣性。

## 大規模語言模型時代（2020-2021）

### 2020年
- **6月**: **[OpenAI]** 發佈 **GPT-3**，參數量1750億，引入了上下文學習技術，能夠執行多種任務。
- **10月**: **[Google]** 發佈 **T5** 模型，統一了多任務訓練框架。

### 2021年
- **1月**: **[OpenAI]** 發佈 **DALL·E**，能夠根據文本描述生成逼真的圖像。
- **1月**: **[OpenAI]** 發佈 **CLIP**，用於分析文本和圖像之間的語義相似性。
- **8月**: **[OpenAI]** 發佈 **Codex**，專為代碼生成設計，成為GitHub Copilot的核心技術。
- **9月**: **[DeepMind]** 發佈 **Gopher**，參數量為2800億，在多數NLP任務上超越了GPT-3。
- **12月**: **[Anthropic]** 公司由前OpenAI研究員成立，開始開發 **Claude** 模型，強調 AI 安全。

## 商業化與應用爆發（2022）

### 2022年
- **1月**: **[Google]** 發佈 **LaMDA** 對話模型。
- **4月**: **[OpenAI]** 發佈 **DALL·E 2**，採用擴散模型進行圖像生成，提供更高分辨率的圖像。
- **4月**: **[DeepMind]** 發佈 **Chinchilla**，證明"計算最優"模型比更大但訓練不足的模型表現更好。
- **9月**: **[OpenAI]** 發佈 **Whisper**，多語言語音識別模型。
- **11月30日**: **[OpenAI]** 發佈 **ChatGPT**，基於GPT-3.5系列，通過RLHF（人類反饋強化學習）進行優化，提供更自然流暢的對話體驗，**標誌著生成式AI進入大眾視野**。

## 多模態與能力擴展（2023）

### 2023年
- **2月**: **[Microsoft]** 基於 **GPT-4** 發佈新版 **Bing搜索**。
- **2月9日**: **[Google]** 發佈 **Bard**，基於LaMDA系列模型。
- **3月**: **[Anthropic]** 發佈 **Claude**，強調安全性和憲法AI理念，使用RLHF技術訓練。
- **3月14日**: **[OpenAI]** 發佈 **GPT-4**，支持多模態輸入，能夠分析文本和圖像。
- **3月24日**: **[OpenAI]** ChatGPT推出 **插件系統**，允許第三方擴展功能。
- **5月**: **[Google]** 宣佈重組AI團隊，**合併DeepMind和Google Brain**為 **Google DeepMind**。
- **7月**: **[Anthropic]** 發佈 **Claude 2**，提升上下文窗口和性能。
- **8月**: **[Anthropic]** Claude 將上下文窗口擴展到 **100K令牌**。
- **9月**: **[Google]** 發佈 **Gemini** 開發路線圖，承諾打造多模態原生模型。
- **12月6日**: **[Google]** 發佈 **Gemini 1.0** 系列，包括Ultra、Pro和Nano三個版本，Ultra在MMLU上首次超越人類專家表現。
- **12月8日**: **[Anthropic]** 將 Claude 的上下文窗口擴展到 **200K令牌**。

## Agent革命開始（2024）

### 2024年
- **1月**: **[OpenAI]** 發佈 **GPT-4 Turbo**，改進了上下文理解，降低了成本。
- **1月29日**: **[Anthropic]** 發佈 **Claude 3** 系列，包括Haiku、Sonnet和Opus三個版本，在多個基準測試中超越了GPT-4。
- **1月31日**: **[DeepSeek]** 發佈 **DeepSeek-V2**，總參數達2360億。
- **2月**: **[OpenAI]** 預告 **Sora**，能夠生成高質量、長時間的視頻內容。
- **2月15日**: **[Google]** 發佈 **Gemini 1.5 Pro**，提供了 **100萬令牌** 的上下文窗口，建立了長上下文處理的新標準。
- **3月**: **[Google]** 發佈 **Gemini Advanced**，搭載 **Gemini Ultra 1.0**，支持更複雜的推理和創意任務。
- **3月6日**: **[零一萬物]** 開源 **Yi-9B** 模型，在代碼和數學能力上表現突出。
- **3月14日**: **[零一萬物]** 正式發佈 **Yi大模型API開放平臺**。
- **5月13日**: **[零一萬物]** 發佈千億參數 **Yi-Large** 閉源模型，同時將中小型號升級為Yi-1.5系列。
- **5月14日**: **[OpenAI]** 發佈 **GPT-4o** ('o' for 'omni')，進一步擴展多模態處理能力，支持文本、音頻和圖像組合輸入，響應速度更快。
- **5月22日**: **[百川智能]** 發佈 **Baichuan4** 基座大模型與"百小應"AI助手。
- **7月18日**: **[OpenAI]** 發佈 **GPT-4o-mini**。
- **8月31日**: **[MiniMax]** 發佈視頻模型 **abab-video-1**、音樂模型 **music-01** 和語音模型 **speech-01**。
- **10月16日**: **[零一萬物]** 發佈預訓練模型 **Yi-Lightning**，性能大幅提升。
- **11月**: **[Anthropic]** 發佈 **Model Context Protocol (MCP)**，為AI系統提供統一的標準，使模型能夠訪問工具和外部數據，開創了Agent發展的新階段。
- **12月5日**: **[OpenAI]** 發佈 **o1** 完整版與 **ChatGPT Pro**，提供更高級的思考能力與200美元/月的高端訂閱。
- **12月6日**: **[OpenAI]** 發佈 **強化微調技術(RFT)**，通過少量示例讓模型學會以全新方式進行推理。
- **12月10日**: **[OpenAI]** 發佈 **Sora Turbo**，提升視頻生成速度。
- **12月13日**: **[DeepSeek]** 發佈多模態理解專家模型 **DeepSeek-VL2**。
- **12月20日**: **[OpenAI]** 預告 **o3** 和 **o3-mini**，優化STEM推理能力。
- **12月26日**: **[DeepSeek]** 正式上線 **DeepSeek-V3** 並同步開源。

## Agent技術大躍進（2025）

### 2025年
- **1月15日**: **[MiniMax]** 發佈 **MiniMax-01系列**，包括基礎語言大模型MiniMax-Text-01和視覺多模態大模型MiniMax-VL-01。
- **1月15日**: **[月之暗面]** 發佈 **moonshot-v1-vision-preview** 多模態圖片理解模型。
- **1月20日**: **[月之暗面]** 發佈 **k1.5** 多模態思考模型，在短思考模式下大幅超越GPT-4o和Claude 3.5 Sonnet。
- **1月20日**: **[DeepSeek]** 正式發佈 **DeepSeek-R1**，性能對標OpenAI o1正式版，並同步開源模型權重。
- **1月23日**: **[OpenAI]** 發佈 **Operator**，由名為Computer-Using Agent (CUA)的新模型驅動，成為OpenAI發佈的第一個Agent。
- **1月24日**: **[百川智能]** 發佈全場景深度思考模型 **Baichuan-M1-preview**、開源醫療增強大模型 **Baichuan-M1-14B**。
- **1月31日**: **[OpenAI]** 發佈 **o3-mini**，在STEM推理方面優化，響應速度比o1更快。
- **2月2日**: **[OpenAI]** 發佈 **Deep Search**，成為OpenAI的第二個Agent。
- **2月**: **[Anthropic]** 發佈 **Claude 3.7 Sonnet**，介紹了混合推理方法，在單一系統中結合快速響應和更深入的逐步"思考"。
- **3月26日**: **[OpenAI]** 宣佈 **採用MCP協議**，表明支持Anthropic的開放標準，用於連接AI代理。
- **4月9日**: **[Google]** 發佈 **Agent-to-Agent (A2A)** 協議，用於AI代理之間的直接通信，與MCP形成互補關係。
- **4月9日**: **[Google DeepMind]** CEO Demis Hassabis 宣佈Google也將支持 **MCP協議**，表明大型AI公司正在某些方面進行合作。
- **4月17日**: **[Google]** 發佈 **Gemini 2.5 Pro**，擁有增強的思維鏈推理能力、100萬令牌的上下文窗口和集成的多模態處理能力。

## AI Agent發展的主要方向

### Anthropic的MCP (Model Context Protocol)
MCP是一個開放標準，定義了應用程序如何向大型語言模型提供結構化上下文。它創建了模型與外部工具、API和數據源之間的安全雙向連接，讓AI助手能夠：
- 從外部源（如API、數據庫和業務平臺）拉取數據
- 向項目管理工具、CRM或版本控制庫等系統發送更新
- 在需要最新實時信息的複雜工作流中運行

**MCP工作原理：**
- **MCP主機**：作為中央樞紐的應用程序，如Claude Desktop
- **MCP服務器**：暴露特定功能的服務
- **MCP客戶端**：通常是由語言模型驅動的應用程序
- **本地數據源**：MCP可以通過本地服務器安全訪問的文件、數據庫
- **遠程服務**：通過API訪問的外部系統

### OpenAI的Agent發展
OpenAI在Agent方面主要專注於三個方向：
- **工具使用與規劃能力**：通過GPT-4發展函數調用和API集成，讓模型可以與外部系統交互
- **自主行動能力**：通過 **Operator** 和 **Deep Search** 等產品探索模型自主使用計算機和執行任務的能力
- **反思與推理**：通過 **o系列模型 (o1, o3, o4-mini)** 增強模型的思考能力，使其能進行多步推理

### Google的A2A (Agent-to-Agent)
Google的A2A是一個開放協議，專為AI代理之間的直接通信而設計。不同於MCP關注代理與工具的連接，A2A解決的是代理之間的通信問題：

**A2A關鍵特性：**
- **代理髮現**：通過 "AgentCard" (類似名片) 機制讓代理互相發現
- **任務協調**：支持從快速操作到長時間運行的流程
- **安全協作**：具有企業級身份驗證和OpenAPI標準
- **基於任務的狀態管理**：更適合管理多代理生態系統

### A2A與MCP的關係
A2A和MCP被設計為互補而非競爭關係：
- **MCP**：標準化 **代理-工具** 接口
- **A2A**：標準化 **代理-代理** 接口

兩者結合提供了構建複雜AI系統的強大雙層模型：MCP提供個體代理能力的構建塊，而A2A提供協作和編排的框架。

### AI Agent未來發展趨勢
1. **專業化與協作**：從單一通用模型轉向多個專業化代理協同工作
2. **自主性增強**：代理能夠獨立規劃和執行復雜任務
3. **多模態融合**：整合文本、圖像、音頻和視頻的無縫處理
4. **長上下文理解**：處理更長曆史記錄的能力增強
5. **安全與隱私**：更強大的安全保障和隱私保護機制
6. **標準化與開放生態**：像MCP和A2A這樣的標準促進互操作性
7. **個性化與記憶能力**：代理具有更持久的記憶和個性化能力

## 結論

大型語言模型的發展進入了新階段，從初始的預訓練語言模型，到提供對話能力的ChatGPT，再到目前的多模態處理和Agent時代。Anthropic通過MCP推動了AI工具標準化，OpenAI快速跟進並拓展了模型的自主能力，Google則通過Gemini的超大上下文窗口和A2A協議推動了上下文處理和代理協作的邊界。

這三家公司的技術路線展現了AI發展的不同方向，但也呈現出逐漸融合的趨勢——特別是在支持開放標準方面的合作。隨著模型能力的不斷增強，以及像MCP和A2A這樣的協議標準化，我們可以預見AI將朝著更高度自主、協作和整合的方向發展，為用戶提供更強大、更自然的智能助手體驗。